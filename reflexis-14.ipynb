{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store Attributes - Store Attributes.csv', 'BITS AIC 2019 - Reflexis Raw Dataset.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pandas import datetime\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Bidirectional, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization, Input, concatenate, K, Reshape, LSTM, CuDNNLSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.losses import *\n",
    "from sklearn.preprocessing import LabelEncoder, minmax_scale, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import backend as K\n",
    "import sklearn\n",
    "import itertools\n",
    "import cv2\n",
    "import scipy\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "%matplotlib inline\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "14ff9d3f58570d36c93af07a1ad3253e2e062d72",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE</th>\n",
       "      <th>TRAFFIC_ACTUAL</th>\n",
       "      <th>TRANSACTIONS_ACTUAL</th>\n",
       "      <th>SALES_ACTUAL</th>\n",
       "      <th>MANAGER_SCHED_HOURS</th>\n",
       "      <th>SYSTEM_SCHED_HOURS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>203</td>\n",
       "      <td>180</td>\n",
       "      <td>37</td>\n",
       "      <td>2243.07</td>\n",
       "      <td>26.00</td>\n",
       "      <td>20.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01</th>\n",
       "      <td>203</td>\n",
       "      <td>49</td>\n",
       "      <td>22</td>\n",
       "      <td>424.50</td>\n",
       "      <td>28.50</td>\n",
       "      <td>29.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-02</th>\n",
       "      <td>203</td>\n",
       "      <td>85</td>\n",
       "      <td>20</td>\n",
       "      <td>878.20</td>\n",
       "      <td>31.50</td>\n",
       "      <td>25.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-03</th>\n",
       "      <td>203</td>\n",
       "      <td>84</td>\n",
       "      <td>25</td>\n",
       "      <td>937.80</td>\n",
       "      <td>25.50</td>\n",
       "      <td>27.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-04</th>\n",
       "      <td>203</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>572.70</td>\n",
       "      <td>25.75</td>\n",
       "      <td>27.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            STORE         ...          SYSTEM_SCHED_HOURS\n",
       "DATE                      ...                            \n",
       "2016-01-31    203         ...                       20.50\n",
       "2016-02-01    203         ...                       29.75\n",
       "2016-02-02    203         ...                       25.50\n",
       "2016-02-03    203         ...                       27.50\n",
       "2016-02-04    203         ...                       27.25\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/BITS AIC 2019 - Reflexis Raw Dataset.csv')\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df = df.set_index('DATE')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e8e750a434c05363beead2021707364a3c00c51f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE</th>\n",
       "      <th>TRAFFIC_ACTUAL</th>\n",
       "      <th>TRANSACTIONS_ACTUAL</th>\n",
       "      <th>SALES_ACTUAL</th>\n",
       "      <th>MANAGER_SCHED_HOURS</th>\n",
       "      <th>SYSTEM_SCHED_HOURS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>203</td>\n",
       "      <td>180</td>\n",
       "      <td>37</td>\n",
       "      <td>2243.07</td>\n",
       "      <td>26.00</td>\n",
       "      <td>20.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01</th>\n",
       "      <td>203</td>\n",
       "      <td>49</td>\n",
       "      <td>22</td>\n",
       "      <td>424.50</td>\n",
       "      <td>28.50</td>\n",
       "      <td>29.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-02</th>\n",
       "      <td>203</td>\n",
       "      <td>85</td>\n",
       "      <td>20</td>\n",
       "      <td>878.20</td>\n",
       "      <td>31.50</td>\n",
       "      <td>25.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-03</th>\n",
       "      <td>203</td>\n",
       "      <td>84</td>\n",
       "      <td>25</td>\n",
       "      <td>937.80</td>\n",
       "      <td>25.50</td>\n",
       "      <td>27.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-04</th>\n",
       "      <td>203</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>572.70</td>\n",
       "      <td>25.75</td>\n",
       "      <td>27.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            STORE         ...          SYSTEM_SCHED_HOURS\n",
       "DATE                      ...                            \n",
       "2016-01-31    203         ...                       20.50\n",
       "2016-02-01    203         ...                       29.75\n",
       "2016-02-02    203         ...                       25.50\n",
       "2016-02-03    203         ...                       27.50\n",
       "2016-02-04    203         ...                       27.25\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "497424e94de85dfb74eab0d1baaec2dee88cc9c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sales =  113805.47\n",
      "Number of records = 121062\n"
     ]
    }
   ],
   "source": [
    "print('Max sales = ', df['SALES_ACTUAL'].max())\n",
    "print('Number of records =', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "3c13bb956d7d2ab56ac7bf3da972b3a3852845bd"
   },
   "outputs": [],
   "source": [
    "\n",
    " #Note: Max profit ever made is the max profit it will ever make, Min possible sales made is the minimum it was ever mode(Since we don't have sufficient parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "1229d2337c9510c2454cdc9b26ac6705785510d2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE</th>\n",
       "      <th>TRAFFIC_ACTUAL</th>\n",
       "      <th>TRANSACTIONS_ACTUAL</th>\n",
       "      <th>SALES_ACTUAL</th>\n",
       "      <th>MANAGER_SCHED_HOURS</th>\n",
       "      <th>SYSTEM_SCHED_HOURS</th>\n",
       "      <th>SALES_DELTA_NORM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>203</td>\n",
       "      <td>180</td>\n",
       "      <td>37</td>\n",
       "      <td>2243.07</td>\n",
       "      <td>26.00</td>\n",
       "      <td>20.50</td>\n",
       "      <td>-0.578366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01</th>\n",
       "      <td>203</td>\n",
       "      <td>49</td>\n",
       "      <td>22</td>\n",
       "      <td>424.50</td>\n",
       "      <td>28.50</td>\n",
       "      <td>29.75</td>\n",
       "      <td>-0.888607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-02</th>\n",
       "      <td>203</td>\n",
       "      <td>85</td>\n",
       "      <td>20</td>\n",
       "      <td>878.20</td>\n",
       "      <td>31.50</td>\n",
       "      <td>25.50</td>\n",
       "      <td>-0.811208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-03</th>\n",
       "      <td>203</td>\n",
       "      <td>84</td>\n",
       "      <td>25</td>\n",
       "      <td>937.80</td>\n",
       "      <td>25.50</td>\n",
       "      <td>27.50</td>\n",
       "      <td>-0.801040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-04</th>\n",
       "      <td>203</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>572.70</td>\n",
       "      <td>25.75</td>\n",
       "      <td>27.25</td>\n",
       "      <td>-0.863325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-05</th>\n",
       "      <td>203</td>\n",
       "      <td>53</td>\n",
       "      <td>8</td>\n",
       "      <td>384.50</td>\n",
       "      <td>30.25</td>\n",
       "      <td>31.50</td>\n",
       "      <td>-0.895431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-06</th>\n",
       "      <td>203</td>\n",
       "      <td>248</td>\n",
       "      <td>43</td>\n",
       "      <td>2110.45</td>\n",
       "      <td>33.00</td>\n",
       "      <td>39.75</td>\n",
       "      <td>-0.600990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-07</th>\n",
       "      <td>203</td>\n",
       "      <td>147</td>\n",
       "      <td>37</td>\n",
       "      <td>2195.30</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.50</td>\n",
       "      <td>-0.586515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-08</th>\n",
       "      <td>203</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.50</td>\n",
       "      <td>23.25</td>\n",
       "      <td>-0.961026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-09</th>\n",
       "      <td>203</td>\n",
       "      <td>78</td>\n",
       "      <td>15</td>\n",
       "      <td>949.50</td>\n",
       "      <td>23.50</td>\n",
       "      <td>23.50</td>\n",
       "      <td>-0.799044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-10</th>\n",
       "      <td>203</td>\n",
       "      <td>75</td>\n",
       "      <td>26</td>\n",
       "      <td>1292.25</td>\n",
       "      <td>36.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>-0.740572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-11</th>\n",
       "      <td>203</td>\n",
       "      <td>99</td>\n",
       "      <td>24</td>\n",
       "      <td>1363.10</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.75</td>\n",
       "      <td>-0.728486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-12</th>\n",
       "      <td>203</td>\n",
       "      <td>126</td>\n",
       "      <td>34</td>\n",
       "      <td>1586.08</td>\n",
       "      <td>30.00</td>\n",
       "      <td>28.75</td>\n",
       "      <td>-0.690446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-13</th>\n",
       "      <td>203</td>\n",
       "      <td>338</td>\n",
       "      <td>58</td>\n",
       "      <td>2756.27</td>\n",
       "      <td>33.00</td>\n",
       "      <td>39.75</td>\n",
       "      <td>-0.490816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-14</th>\n",
       "      <td>203</td>\n",
       "      <td>224</td>\n",
       "      <td>31</td>\n",
       "      <td>1159.53</td>\n",
       "      <td>20.50</td>\n",
       "      <td>25.50</td>\n",
       "      <td>-0.763214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-15</th>\n",
       "      <td>203</td>\n",
       "      <td>264</td>\n",
       "      <td>40</td>\n",
       "      <td>1768.50</td>\n",
       "      <td>25.25</td>\n",
       "      <td>35.50</td>\n",
       "      <td>-0.659326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-16</th>\n",
       "      <td>203</td>\n",
       "      <td>116</td>\n",
       "      <td>23</td>\n",
       "      <td>989.55</td>\n",
       "      <td>24.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>-0.792212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-17</th>\n",
       "      <td>203</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>1283.14</td>\n",
       "      <td>24.50</td>\n",
       "      <td>32.50</td>\n",
       "      <td>-0.742127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-18</th>\n",
       "      <td>203</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "      <td>767.25</td>\n",
       "      <td>24.50</td>\n",
       "      <td>20.00</td>\n",
       "      <td>-0.830136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19</th>\n",
       "      <td>203</td>\n",
       "      <td>200</td>\n",
       "      <td>37</td>\n",
       "      <td>1629.45</td>\n",
       "      <td>30.00</td>\n",
       "      <td>31.75</td>\n",
       "      <td>-0.683047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-20</th>\n",
       "      <td>203</td>\n",
       "      <td>281</td>\n",
       "      <td>67</td>\n",
       "      <td>2788.10</td>\n",
       "      <td>32.00</td>\n",
       "      <td>35.25</td>\n",
       "      <td>-0.485386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-21</th>\n",
       "      <td>203</td>\n",
       "      <td>161</td>\n",
       "      <td>31</td>\n",
       "      <td>1612.10</td>\n",
       "      <td>25.50</td>\n",
       "      <td>25.50</td>\n",
       "      <td>-0.686007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-22</th>\n",
       "      <td>203</td>\n",
       "      <td>64</td>\n",
       "      <td>19</td>\n",
       "      <td>515.20</td>\n",
       "      <td>24.50</td>\n",
       "      <td>23.25</td>\n",
       "      <td>-0.873134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-23</th>\n",
       "      <td>203</td>\n",
       "      <td>60</td>\n",
       "      <td>19</td>\n",
       "      <td>1133.99</td>\n",
       "      <td>25.00</td>\n",
       "      <td>23.50</td>\n",
       "      <td>-0.767571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-24</th>\n",
       "      <td>203</td>\n",
       "      <td>70</td>\n",
       "      <td>21</td>\n",
       "      <td>650.60</td>\n",
       "      <td>24.00</td>\n",
       "      <td>24.50</td>\n",
       "      <td>-0.850036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-25</th>\n",
       "      <td>203</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>697.20</td>\n",
       "      <td>23.75</td>\n",
       "      <td>24.75</td>\n",
       "      <td>-0.842086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-26</th>\n",
       "      <td>203</td>\n",
       "      <td>129</td>\n",
       "      <td>34</td>\n",
       "      <td>1568.70</td>\n",
       "      <td>23.75</td>\n",
       "      <td>23.75</td>\n",
       "      <td>-0.693411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-27</th>\n",
       "      <td>203</td>\n",
       "      <td>344</td>\n",
       "      <td>64</td>\n",
       "      <td>4360.53</td>\n",
       "      <td>34.00</td>\n",
       "      <td>41.25</td>\n",
       "      <td>-0.217135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-28</th>\n",
       "      <td>203</td>\n",
       "      <td>151</td>\n",
       "      <td>29</td>\n",
       "      <td>1420.80</td>\n",
       "      <td>22.50</td>\n",
       "      <td>20.50</td>\n",
       "      <td>-0.718642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>203</td>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "      <td>816.28</td>\n",
       "      <td>24.00</td>\n",
       "      <td>23.25</td>\n",
       "      <td>-0.821771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-26</th>\n",
       "      <td>7903</td>\n",
       "      <td>311</td>\n",
       "      <td>76</td>\n",
       "      <td>3715.98</td>\n",
       "      <td>33.50</td>\n",
       "      <td>28.50</td>\n",
       "      <td>-0.882518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-27</th>\n",
       "      <td>7903</td>\n",
       "      <td>956</td>\n",
       "      <td>187</td>\n",
       "      <td>9374.42</td>\n",
       "      <td>67.00</td>\n",
       "      <td>72.50</td>\n",
       "      <td>-0.703624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-28</th>\n",
       "      <td>7903</td>\n",
       "      <td>503</td>\n",
       "      <td>84</td>\n",
       "      <td>3883.04</td>\n",
       "      <td>39.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>-0.877236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-29</th>\n",
       "      <td>7903</td>\n",
       "      <td>217</td>\n",
       "      <td>66</td>\n",
       "      <td>2259.89</td>\n",
       "      <td>38.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>-0.928553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-30</th>\n",
       "      <td>7903</td>\n",
       "      <td>169</td>\n",
       "      <td>50</td>\n",
       "      <td>2389.24</td>\n",
       "      <td>27.50</td>\n",
       "      <td>22.00</td>\n",
       "      <td>-0.924463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>7903</td>\n",
       "      <td>157</td>\n",
       "      <td>43</td>\n",
       "      <td>1713.73</td>\n",
       "      <td>30.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>-0.945820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>7903</td>\n",
       "      <td>215</td>\n",
       "      <td>63</td>\n",
       "      <td>3450.06</td>\n",
       "      <td>36.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>-0.890925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-02</th>\n",
       "      <td>7903</td>\n",
       "      <td>311</td>\n",
       "      <td>85</td>\n",
       "      <td>4421.01</td>\n",
       "      <td>54.50</td>\n",
       "      <td>26.50</td>\n",
       "      <td>-0.860228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-03</th>\n",
       "      <td>7903</td>\n",
       "      <td>888</td>\n",
       "      <td>199</td>\n",
       "      <td>9410.24</td>\n",
       "      <td>117.50</td>\n",
       "      <td>45.00</td>\n",
       "      <td>-0.702491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>7903</td>\n",
       "      <td>385</td>\n",
       "      <td>85</td>\n",
       "      <td>6345.65</td>\n",
       "      <td>40.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>-0.799379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>7903</td>\n",
       "      <td>146</td>\n",
       "      <td>42</td>\n",
       "      <td>1683.31</td>\n",
       "      <td>33.00</td>\n",
       "      <td>29.50</td>\n",
       "      <td>-0.946781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>7903</td>\n",
       "      <td>154</td>\n",
       "      <td>53</td>\n",
       "      <td>2590.64</td>\n",
       "      <td>31.00</td>\n",
       "      <td>28.25</td>\n",
       "      <td>-0.918096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-07</th>\n",
       "      <td>7903</td>\n",
       "      <td>180</td>\n",
       "      <td>58</td>\n",
       "      <td>2557.31</td>\n",
       "      <td>28.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>-0.919150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-08</th>\n",
       "      <td>7903</td>\n",
       "      <td>201</td>\n",
       "      <td>73</td>\n",
       "      <td>3063.63</td>\n",
       "      <td>33.00</td>\n",
       "      <td>28.25</td>\n",
       "      <td>-0.903142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-09</th>\n",
       "      <td>7903</td>\n",
       "      <td>319</td>\n",
       "      <td>107</td>\n",
       "      <td>4425.47</td>\n",
       "      <td>36.50</td>\n",
       "      <td>30.75</td>\n",
       "      <td>-0.860087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-10</th>\n",
       "      <td>7903</td>\n",
       "      <td>1092</td>\n",
       "      <td>252</td>\n",
       "      <td>12632.69</td>\n",
       "      <td>66.00</td>\n",
       "      <td>71.50</td>\n",
       "      <td>-0.600612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-11</th>\n",
       "      <td>7903</td>\n",
       "      <td>466</td>\n",
       "      <td>76</td>\n",
       "      <td>4960.04</td>\n",
       "      <td>39.50</td>\n",
       "      <td>36.75</td>\n",
       "      <td>-0.843186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-12</th>\n",
       "      <td>7903</td>\n",
       "      <td>195</td>\n",
       "      <td>52</td>\n",
       "      <td>3363.96</td>\n",
       "      <td>31.50</td>\n",
       "      <td>28.25</td>\n",
       "      <td>-0.893647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-13</th>\n",
       "      <td>7903</td>\n",
       "      <td>163</td>\n",
       "      <td>47</td>\n",
       "      <td>2183.39</td>\n",
       "      <td>31.50</td>\n",
       "      <td>28.25</td>\n",
       "      <td>-0.930971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-14</th>\n",
       "      <td>7903</td>\n",
       "      <td>163</td>\n",
       "      <td>42</td>\n",
       "      <td>2243.87</td>\n",
       "      <td>29.50</td>\n",
       "      <td>31.50</td>\n",
       "      <td>-0.929059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-15</th>\n",
       "      <td>7903</td>\n",
       "      <td>226</td>\n",
       "      <td>88</td>\n",
       "      <td>4531.76</td>\n",
       "      <td>42.50</td>\n",
       "      <td>30.50</td>\n",
       "      <td>-0.856726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-16</th>\n",
       "      <td>7903</td>\n",
       "      <td>487</td>\n",
       "      <td>194</td>\n",
       "      <td>10521.07</td>\n",
       "      <td>42.50</td>\n",
       "      <td>39.50</td>\n",
       "      <td>-0.667372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-17</th>\n",
       "      <td>7903</td>\n",
       "      <td>1418</td>\n",
       "      <td>413</td>\n",
       "      <td>25253.43</td>\n",
       "      <td>56.50</td>\n",
       "      <td>61.75</td>\n",
       "      <td>-0.201602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-18</th>\n",
       "      <td>7903</td>\n",
       "      <td>976</td>\n",
       "      <td>204</td>\n",
       "      <td>11501.24</td>\n",
       "      <td>55.75</td>\n",
       "      <td>35.75</td>\n",
       "      <td>-0.636383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-19</th>\n",
       "      <td>7903</td>\n",
       "      <td>645</td>\n",
       "      <td>168</td>\n",
       "      <td>9632.17</td>\n",
       "      <td>46.50</td>\n",
       "      <td>20.75</td>\n",
       "      <td>-0.695475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-20</th>\n",
       "      <td>7903</td>\n",
       "      <td>151</td>\n",
       "      <td>45</td>\n",
       "      <td>2670.65</td>\n",
       "      <td>26.00</td>\n",
       "      <td>23.25</td>\n",
       "      <td>-0.915566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-21</th>\n",
       "      <td>7903</td>\n",
       "      <td>162</td>\n",
       "      <td>36</td>\n",
       "      <td>1422.05</td>\n",
       "      <td>31.00</td>\n",
       "      <td>24.50</td>\n",
       "      <td>-0.955041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-22</th>\n",
       "      <td>7903</td>\n",
       "      <td>265</td>\n",
       "      <td>80</td>\n",
       "      <td>3728.43</td>\n",
       "      <td>35.00</td>\n",
       "      <td>20.75</td>\n",
       "      <td>-0.882124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-23</th>\n",
       "      <td>7903</td>\n",
       "      <td>472</td>\n",
       "      <td>125</td>\n",
       "      <td>6558.13</td>\n",
       "      <td>40.00</td>\n",
       "      <td>33.25</td>\n",
       "      <td>-0.792662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-24</th>\n",
       "      <td>7903</td>\n",
       "      <td>1398</td>\n",
       "      <td>274</td>\n",
       "      <td>15390.42</td>\n",
       "      <td>56.00</td>\n",
       "      <td>56.25</td>\n",
       "      <td>-0.513425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121062 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            STORE        ...         SALES_DELTA_NORM\n",
       "DATE                     ...                         \n",
       "2016-01-31    203        ...                -0.578366\n",
       "2016-02-01    203        ...                -0.888607\n",
       "2016-02-02    203        ...                -0.811208\n",
       "2016-02-03    203        ...                -0.801040\n",
       "2016-02-04    203        ...                -0.863325\n",
       "2016-02-05    203        ...                -0.895431\n",
       "2016-02-06    203        ...                -0.600990\n",
       "2016-02-07    203        ...                -0.586515\n",
       "2016-02-08    203        ...                -0.961026\n",
       "2016-02-09    203        ...                -0.799044\n",
       "2016-02-10    203        ...                -0.740572\n",
       "2016-02-11    203        ...                -0.728486\n",
       "2016-02-12    203        ...                -0.690446\n",
       "2016-02-13    203        ...                -0.490816\n",
       "2016-02-14    203        ...                -0.763214\n",
       "2016-02-15    203        ...                -0.659326\n",
       "2016-02-16    203        ...                -0.792212\n",
       "2016-02-17    203        ...                -0.742127\n",
       "2016-02-18    203        ...                -0.830136\n",
       "2016-02-19    203        ...                -0.683047\n",
       "2016-02-20    203        ...                -0.485386\n",
       "2016-02-21    203        ...                -0.686007\n",
       "2016-02-22    203        ...                -0.873134\n",
       "2016-02-23    203        ...                -0.767571\n",
       "2016-02-24    203        ...                -0.850036\n",
       "2016-02-25    203        ...                -0.842086\n",
       "2016-02-26    203        ...                -0.693411\n",
       "2016-02-27    203        ...                -0.217135\n",
       "2016-02-28    203        ...                -0.718642\n",
       "2016-02-29    203        ...                -0.821771\n",
       "...           ...        ...                      ...\n",
       "2018-01-26   7903        ...                -0.882518\n",
       "2018-01-27   7903        ...                -0.703624\n",
       "2018-01-28   7903        ...                -0.877236\n",
       "2018-01-29   7903        ...                -0.928553\n",
       "2018-01-30   7903        ...                -0.924463\n",
       "2018-01-31   7903        ...                -0.945820\n",
       "2018-02-01   7903        ...                -0.890925\n",
       "2018-02-02   7903        ...                -0.860228\n",
       "2018-02-03   7903        ...                -0.702491\n",
       "2018-02-04   7903        ...                -0.799379\n",
       "2018-02-05   7903        ...                -0.946781\n",
       "2018-02-06   7903        ...                -0.918096\n",
       "2018-02-07   7903        ...                -0.919150\n",
       "2018-02-08   7903        ...                -0.903142\n",
       "2018-02-09   7903        ...                -0.860087\n",
       "2018-02-10   7903        ...                -0.600612\n",
       "2018-02-11   7903        ...                -0.843186\n",
       "2018-02-12   7903        ...                -0.893647\n",
       "2018-02-13   7903        ...                -0.930971\n",
       "2018-02-14   7903        ...                -0.929059\n",
       "2018-02-15   7903        ...                -0.856726\n",
       "2018-02-16   7903        ...                -0.667372\n",
       "2018-02-17   7903        ...                -0.201602\n",
       "2018-02-18   7903        ...                -0.636383\n",
       "2018-02-19   7903        ...                -0.695475\n",
       "2018-02-20   7903        ...                -0.915566\n",
       "2018-02-21   7903        ...                -0.955041\n",
       "2018-02-22   7903        ...                -0.882124\n",
       "2018-02-23   7903        ...                -0.792662\n",
       "2018-02-24   7903        ...                -0.513425\n",
       "\n",
       "[121062 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.groupby('STORE')\n",
    "_temp = []\n",
    "scalar = MinMaxScaler(feature_range=(-1,1))\n",
    "for store_no, store_data in df1:\n",
    "    data = df1.get_group(store_no).copy()\n",
    "    #data['SALES_DELTA_NORM'] = minmax_scale(data['SALES_ACTUAL'], feature_range=(-1, 1))\n",
    "    \n",
    "    data['SALES_DELTA_NORM'] = scalar.fit_transform(data['SALES_ACTUAL'].values.reshape(-1,1))\n",
    "    for i in data['SALES_DELTA_NORM'].values:\n",
    "        _temp.append(i)\n",
    "df['SALES_DELTA_NORM'] = _temp\n",
    "\n",
    "df1 = df.groupby('STORE')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "865fc12b612773e22d7b485be0836610de2e578b"
   },
   "outputs": [],
   "source": [
    "class MetricsCheckpoint(Callback):\n",
    "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
    "    def __init__(self, savepath):\n",
    "        super(MetricsCheckpoint, self).__init__()\n",
    "        self.savepath = savepath\n",
    "        self.history = {}\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        np.save(self.savepath, self.history)\n",
    "        \n",
    "class MetricsCheckpoint(Callback):\n",
    "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
    "    def __init__(self, savepath):\n",
    "        super(MetricsCheckpoint, self).__init__()\n",
    "        self.savepath = savepath\n",
    "        self.history = {}\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        np.save(self.savepath, self.history)\n",
    "\n",
    "def plotKerasLearningCurve():\n",
    "    plt.figure(figsize=(10,5))\n",
    "    metrics = np.load('logs.npy')[()]\n",
    "    filt = ['loss'] # try to add 'loss' to see the loss learning curve\n",
    "    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n",
    "        #l = metrics[k]\n",
    "        l = np.sqrt(scalar.inverse_transform(np.array(metrics[k]).reshape(-1,1)).ravel()).tolist()\n",
    "        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n",
    "        x = np.argmin(l)\n",
    "        y = float(l[x])\n",
    "        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n",
    "        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n",
    "    plt.legend(loc=4)\n",
    "    plt.axis([0, None, None, None]);\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('RMSE')\n",
    "    \n",
    "def plot_learning_curve(history):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('./loss_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "def f1(dfin, store_no, epochs):\n",
    "#def f1(data, store_no, epochs):\n",
    "    #predict sales given date, store number, system scheduled hours and manager scheduled hours\n",
    "    data = dfin.get_group(store_no)\n",
    "    batch_size = 8\n",
    "    scalar.fit(data['SALES_ACTUAL'].values.reshape(-1,1))\n",
    "    #batch_size = 256\n",
    "    split_date = '2018-01-01'\n",
    "    X_train, Y_train, X_test, Y_test = data[data.index < split_date][['STORE','MANAGER_SCHED_HOURS','SYSTEM_SCHED_HOURS']], data[data.index < split_date]['SALES_DELTA_NORM'], data[data.index> split_date][['STORE','MANAGER_SCHED_HOURS','SYSTEM_SCHED_HOURS']], data[data.index > split_date]['SALES_DELTA_NORM']\n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(data[['STORE','MANAGER_SCHED_HOURS','SYSTEM_SCHED_HOURS']],data['SALES_ACTUAL'], shuffle=False)\n",
    "    X_train = X_train.values.reshape(X_train.shape[0],1,X_train.shape[1])\n",
    "    X_test = X_test.values.reshape(X_test.shape[0],1,X_test.shape[1])\n",
    "    input_shape = (1,X_test.shape[2])\n",
    "    num_classes = 1\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True, input_shape=input_shape)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Bidirectional(CuDNNLSTM(64)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_classes, activation='tanh'))\n",
    "    model.compile(loss=root_mean_squared_error, optimizer = Adam(lr=0.01), metrics=[root_mean_squared_error])\n",
    "    history = model.fit(X_train, Y_train.values, epochs = epochs, steps_per_epoch=int(len(X_train) / batch_size), validation_data=(X_test, Y_test.values), validation_steps=max(int(len(X_test)/batch_size),1), callbacks = [MetricsCheckpoint('logs')])\n",
    "    #history = model.fit(X_train, Y_train.values, epochs = epochs, batch_size = batch_size, validation_data=(X_test, Y_test.values), callbacks = [MetricsCheckpoint('logs')])\n",
    "    score = model.evaluate(X_test, Y_test.values)\n",
    "    plotKerasLearningCurve()\n",
    "    plt.show()  \n",
    "    plot_learning_curve(history)\n",
    "    plt.show()\n",
    "    return model\n",
    "    \n",
    "#def f2(data):\n",
    "    #maximize sales, by adjusting sys_hrs and mgr_hrs for f1, output (sys_hrs,mgr_hrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "1b7a114960d9a1e012c9fc7411f10ef6d60a9c6a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 701 samples, validate on 54 samples\n",
      "Epoch 1/50\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 0.1578 - root_mean_squared_error: 0.1578 - val_loss: 0.0606 - val_root_mean_squared_error: 0.0606\n",
      "Epoch 2/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 3/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 4/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 5/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 6/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 7/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 8/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 9/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 10/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 11/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 12/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 13/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 14/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 15/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 16/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 17/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 18/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 19/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 20/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 21/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 22/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 23/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 24/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 25/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 26/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 27/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 28/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 29/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 30/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 31/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 32/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 33/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 34/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 35/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 36/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 37/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 38/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 39/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 40/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 41/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 42/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 43/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 44/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 45/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 46/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 47/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 48/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 49/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "Epoch 50/50\n",
      "87/87 [==============================] - 1s 8ms/step - loss: inf - root_mean_squared_error: inf - val_loss: inf - val_root_mean_squared_error: inf\n",
      "54/54 [==============================] - 0s 98us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-35f7770f1069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m203\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-6ddbfe43a5b5>\u001b[0m in \u001b[0;36mf1\u001b[0;34m(dfin, store_no, epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#history = model.fit(X_train, Y_train.values, epochs = epochs, batch_size = batch_size, validation_data=(X_test, Y_test.values), callbacks = [MetricsCheckpoint('logs')])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mplotKerasLearningCurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8d242d90971b>\u001b[0m in \u001b[0;36mplotKerasLearningCurve\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#l = metrics[k]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'r'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'val'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'val'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0;32m--> 413\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains infinity or a value too large for dtype('float64')."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lstm_model = f1(df1, 203, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "35523a8e5f20e2907bb73f82c764bdcb2766b55e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9ce36cf146ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mx_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstore_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mls1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwpd\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm_model' is not defined"
     ]
    }
   ],
   "source": [
    "#General scheme for feature visaulization is to iterate over all possible inputs\n",
    "store_no = 203\n",
    "ls1=[]\n",
    "ls2=[]\n",
    "dfstore = df1.get_group(store_no)\n",
    "scalar.fit(dfstore['SALES_ACTUAL'].values.reshape(-1,1))\n",
    "wpd = 12\n",
    "for i in range(wpd*7*4+1):\n",
    "    x_in = np.array([[store_no, float(i)/4.0, float(i)/4.0]])\n",
    "    x_in = x_in.reshape(x_in.shape[0], 1, x_in.shape[1])\n",
    "    ls1.append(scalar.inverse_transform(lstm_model.predict(x_in)).tolist()[0][0])\n",
    "\n",
    "for j in range(wpd*7*4+1):\n",
    "    x_in = np.array([[store_no, float(j)/4.0, float(ls1.index(max(ls1)))/4.0]])\n",
    "    x_in = x_in.reshape(x_in.shape[0], 1, x_in.shape[1])\n",
    "    ls2.append(scalar.inverse_transform(lstm_model.predict(x_in)).tolist()[0][0])\n",
    "xrange = np.arange(0,wpd*7+0.25,0.25)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "5d7235f1575d0bf6f874f57398daa21d96c8699a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ed6446cbef92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'System Hours'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Profits'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(24,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(xrange, ls1)\n",
    "plt.xlabel('System Hours')\n",
    "plt.ylabel('Profits')\n",
    "x = np.argmax(ls1)\n",
    "y = ls1[x]\n",
    "plt.scatter(x/4,y, lw=0, alpha=0.25, s=100, c='b')\n",
    "plt.text(x/4, y, 'Max profit at\\n{} = {:.4f}'.format(x/4,y), size='15', color='b')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xrange, ls2)\n",
    "plt.xlabel('Manager Hours')\n",
    "plt.ylabel('Profits')\n",
    "x = np.argmax(ls2)\n",
    "y = ls2[x]\n",
    "plt.scatter(x/4,y, lw=0, alpha=0.25, s=100, c='b')\n",
    "plt.text(x/4, y, 'Max profit at\\n{} = {:.4f}'.format(x/4,y), size='15', color='b')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
